[segment]
tool = jieba
mode = char
embedding = word2vec

[back]
tf = 1.10.0
torch = 0.4.0

[random]
seed = 42
train_seed = 42
pre_seed = 16

[model]
tokenizer = cache/tokenizer_char.pickle
maxlen = 1200
dir = model/bigru_char_combine/
pre = conf/pre_bigru_combine.json
max_features = 20000
batch_size = 128
epochs = 15

[data]
train_download = data/ai_challenger_sentiment_analysis_trainingset_20180816/sentiment_analysis_trainingset.csv
val_download = data/ai_challenger_sentiment_analysis_validationset_20180816/sentiment_analysis_validationset.csv
test_download = data/ai_challenger_sentiment_analysis_testa_20180816/sentiment_analysis_testa.csv
train = preprocess/train_char.csv
val = preprocess/val_char.csv
test = preprocess/test_char.csv
submit = submit/bigru/bigru_char.csv
submit_prob = submit/bigru/bigru_char_prob.csv
src_field = content
fields = location_traffic_convenience,location_distance_from_business_district,location_easy_to_find,service_wait_time,service_waiters_attitude,service_parking_convenience,service_serving_speed,price_level,price_cost_effective,price_discount,environment_decoration,environment_noise,environment_space,environment_cleaness,dish_portion,dish_taste,dish_look,dish_recommendation,others_overall_experience,others_willing_to_consume_again
combine = yes

[embedding]
word2vec = word2vec/chars.vector
glove = word2vec/chars_vectors_w10.txt
size = 100
window = 10

[label]
vocab = -2,-1,0,1